\documentclass{sigplanconf}

\pagestyle{plain}
\usepackage{amsmath,stmaryrd,natbib,listings}
\usepackage[usenames,dvipsnames]{color}

\newcommand{\Scribtexttt}[1]{{\texttt{#1}}}
\newcommand{\SColorize}[2]{\color{#1}{#2}}
\newcommand{\inColor}[2]{{\Scribtexttt{\SColorize{#1}{#2}}}}
\definecolor{PaleBlue}{rgb}{0.90,0.90,1.0}
\newcommand{\rackett}[1]{\inColor{black}{#1}}
\input{local-macros}

\title{Concrete Semantics for Pushdown Analysis:\\The Essence of Summarization}
\authorinfo{J. Ian Johnson and David Van Horn}{Northeastern University}{\{ianj,dvanhorn\}@ccs.neu.edu}

\usepackage{hyperref}
\begin{document}
\maketitle

% Outline:
% Introduction.
% High level
% PDCFA
% - Concrete
% - Abstract
% CFA2
% - Concrete
% - Abstract
% WCM machine
% - Concrete
% - Abstract
% Related Work
% Future Work
% Conclusion

\begin{abstract}
Pushdown analysis is better than finite-state analysis in precision and
performance. Why then have we not seen total widespread adoption of these
techniques? For one, the known techniques are technically burdened and
difficult to understand or extend. Control structure of the programming language
gets pulled into the model of computation, which makes extensions to
non-pushdown control structures, such as \rackett{call/cc} or \rackett{shift} and
\rackett{reset}, non-trivial.

We show a derivational approach to abstract interpretation that yields
transparently sound static analyses that can precisely match calls and
returns when applied to well-known abstract machines. We show that
adding memoization and segmenting the continuation into bounded pieces
leads to machines that abstract to static analyses for context-free
reachability by simply bounding the stores. This technique allows us
to derive existing, more technically involved analyses, and a novel
pushdown analysis for delimited, composable control.
\end{abstract}

\section{Introduction}

Programs in higher-order languages heavily use function calls and
method dispatch as part of their control flow. Until recently, flow
analyses for higher-order languages could not handle return flow
precisely \citep{ianjohnson:vardoulakis-lmcs11,
  ianjohnson:earl2010pdcfa}, which leads to several spurious paths
(and thus false positives) due to the pervasiveness of function/method
calls and subsequent returns. These works, called CFA2 and PDCFA
respectively, use pushdown automata as their approximation's target
model of computation. They are hence called ``pushdown
analyses.''\footnote{We will refer to the classic finite model
  analyses as ``regular analyses'' after regular languages.} CFA2 and
PDCFA have difficult details to easily apply to an off-the-shelf
semantics --- especially if they feature non-local control transfer
that breaks the pushdown model.

There is a systematic process for transforming off-the-shelf
programming language semantics into a form amenable to \emph{regular}
analysis that has been widely applied with great success to production
programming languages ~\citet{dvanhorn:VanHorn2010Abstracting} (a
technique called abstracting abstract machines, or AAM). A
contribution of this paper is a systematic process to construct
\emph{pushdown} analyses of programming languages, due to the
precision (and often performance) benefits.

Testing new ideas in analysis for improving precision or increasing
performance can be a difficult venture. We contend that the machinery
that we employ in the abstract should have a concrete counterpart that
maintains the meaning of the language so that we can see their effect
without introducing approximations which might mask correctness
issues. Abstraction should be a simple process that is ``obviously
correct.'' The framework we present in this paper is applicable in the
concrete such that a point-wise abstraction leads to the pushdown
analyses in the literature.

This paper gives a common, simple framework to derive both CFA2 and
PDCFA using a recipe to apply pushdown analysis techniques to
arbitrary semantics in a manner similar to AAM. To exercise the recipe further, 
we give a novel analysis for delimited, composable control. \\

\section{What to expect}

CFA2 uses a technique called ``summarization'' from
\citet[Chapter 7]{local:muchnick:jones:flow-analysis:1981}, which is synonymous with the
$\epsilon$-closure graph that PDCFA constructs. Summarization
algorithms need not be restricted to languages with well-bracketed
calls and returns. We can adopt the technique for higher precision in
the common case but still handle difficult cases such as first-class
control. This was shown for the
\rackett{call-with-current-continuation} (a.k.a. \rackett{call/cc})
operator in ~\citet{ianjohnson:Vardoulakis2011Pushdown}. This
impressive work illuminated the fact that we can harness the enhanced
technology of pushdown analyses in non-pushdown models of
computation. Doing this sacrifices call/return matching in the general
case, but in practice the precision is much better than the
alternative regular model that, say, AAM would provide.

A downside of the work providing \rackett{call/cc} is that it was an
algorithmic change to the already complicated CFA2 --- there was no
recipe for how to do this for one's favorite control operator. This
paper seeks to do just that with an operational view of what
summarization is, in essence. In other words, we give a concrete
semantics to the tricks that the analyses in the literature use in the
abstract, and maintain the original meaning of the language. We also
give intuitive analogies to well-established ideas/techniques so that
the working semanticist can write a pushdown analysis for her
language. In order to demonstrate the applicability of this viewpoint,
we show a new analysis for a language with composable control. All of
the semantics modeled in this paper are implemented in full detail in
PLT Redex~\citep{ianjohnson:Felleisen:2009:SEP:1795772} and available
online\footnote{\url{http://github.com/ianj/concrete-summaries}}.

There are common underpinnings of PDCFA and CFA2 that can be embodied
as concrete machinery in the programming language semantics: 
\begin{enumerate}
\item{breaking the recursive structure of continuations by indirecting them through a
table with appropriately precise keys, and}
\item{memoization.}
\end{enumerate}
Once the machine semantics is in this form, simple point-wise
abstraction leads to the summarization algorithms that we see in the
literature.

The remaining sections of the paper are
\begin{itemize}
\item{section \ref{sec:pdcfa}: we derive a cousin of PDCFA}
\item{section \ref{sec:cfa2}: we make additions to the previous semantics to get a direct-style CFA2 without first-class control}
\item{section \ref{sec:sr}: we give a novel analysis of delimited and composable first-class control.}
\end{itemize}

\section{Deriving PDCFA}
\label{sec:pdcfa}

PDCFA does not have some orthogonal semantic components that CFA2
features to improve precision and is thus the simpler of the two. The
key to their method is to notice in languages without stack-capturing
features, the continuation is only modified a frame at a time. If the
state space without the stack is finite (and it can be made finite
with an approximation ala AAM), then the model falls directly into the
realm of pushdown systems. They thus recast the problem in terms of
pushdown systems. The whole of their machinery is then computing the
pushdown system on-the-fly, only considering states reachable from the
root (initial) state. They call the main data-structure for this a
Dyck state graph. To match nodes that push frames with later nodes
following the pop of that frame, they additionally compute an
$\epsilon$-closure graph, which keeps edges between nodes that have a
net-zero stack change path between them. We skip the detour to
pushdown systems altogether and show an operational understanding of
the resulting analysis.

We start by deriving PDCFA from an operational semantics for the
untyped lambda calculus (figure \ref{fig:base-semantics}). The
semantic spaces for the machine follow.

\begin{align*}
  \mexpr \in \Expr &= \svar{\mvar} \mid \sapp{\mexpr}{\mexpr} \mid \slam{\mvar}{\mexpr} \\
  \mstate \in \State &= (\Expr \times \Env) \times \Store \times \Kont \\
  \mval \in \Value &::= \vclo{\slam{\mvar}{\mexpr}}{\menv} \\
  \mkont \in \Kont &::= \kmt \mid \kar{\mexpr,\menv,\mkont} \mid \kfn{\mval,\mkont} \\
  \menv \in \Env &= \Var \to \Addr \\
  \mstore \in \Store &= \Addr \to \wp(\Value) \\
  \mvar \in \Var &\text{ an infinite set} \\
  \maddr \in \Addr &\text{ an infinite set}
\end{align*}

The $\Addr$ space is what controls the precision of the model. For a
concrete semantics, we require the allocation meta-function $\alloc :
\State \to \Addr$ to return fresh addresses ($\maddr \notin
\dom(\mstore)$), but any choice of address is sound. If $\alloc$ only
uses addresses from a finite subset of $\Addr$, then the state space
without continuations is finite, and the space of contination frames
is finite (thus the pushdown system interpretation is apt). This
meta-function along with the commitment to having no recursive
data-structures is the key to the AAM technique. All recursion can be
expressed by indirecting through addresses into a store --- tying
Landin's knot. We borrow this technique on top of our own.

\begin{figure}
  \centering
  $\mstate \stepto \mstate' \text{ where } \maddr = \alloc(\mstate)$ \\
  \begin{tabular}{r|l}
    \hline
% variable lookup
    $\tpl{(\svar\mvar, \menv), \mstore, \mkont}$
    &
    $\tpl{\mval,\mstore,\mkont}$ if $\mval \in \mstore(\menv(\mvar))$
    \\
% application
    $\tpl{(\sapp{\mexpri0}{\mexpri1}, \menv), \mstore, \mkont}$
    &
    $\tpl{(\mexpri0, \menv), \mstore, \kar{\mexpri1,\menv,\mkont}}$
    \\
% argument evaluation
    $\tpl{\mval, \mstore, \kar{\mexpr,\menv,\mkont}}$
    &
    $\tpl{(\mexpr, \menv), \mstore, \kfn{\mval, \mkont}}$
    \\
% function call
    $\tpl{\mval,\mstore,\kfn{\vclo{\slam{\mvar}{\mexpr}}{\menv}, \mkont}}$
    &
    $\tpl{(\mexpr, \extm{\menv}{\mvar}{\maddr}), \joinone{\mstore}{\maddr}{\mval}, \mkont}$
  \end{tabular}
  \caption{The CESK machine}
  \label{fig:base-semantics}
\end{figure}

\paragraph{Tracking return points:} the magic of the method is in
keeping a table of continuation segments for each function and store
pair, where the continuation is segmented at function boundaries. This
closely mirrors the technique of store-allocating continuations used
in AAM. Instead of deferring to the table (store) for the remainder of
a continuation for each frame, we only have indirections at function
call boundaries (see rules 4 and 5 in figure
\ref{fig:summary-semantics} for saving and restoring
continuations). Note that since AAM is about finitizing the state
space, this step itself fits within the AAM method, since
continuations within functions truncated at call boundaries are
bounded by the nesting depth of those functions, thereby making the
continuation space finite. In the monovariant case, the number of
continuations is still linear in the size of the program.

The tail of a continuation in the context of a function will contain
the stack-less context in which the function was called, in order to
link up with the proper call-site(s). The context includes the
function (or unique label of the function), the environment, and the
store. Notice that this choice can be viewed as a special allocation
strategy for continuations in the AAM viewpoint, but we separate the
table of continuations from the store since continuations themselves
contain stores in $\rt$ frames --- this leads to a recursive
data-structure that we are trying to avoid. The store is an important
ingredient to maintaining enough precision to keep a pushdown
abstraction. The semantic spaces for the machine are modified thusly:

\begin{align*}
  \mstate \in \State &= (\Expr \times \Env) \times \Store \times \Kont \times \KTab \times \Memo \\
  \mkont \in \Kont &::= \kmt
                     \mid \kar{\mexpr,\menv,\mkont} 
                     \mid \kfn{\mval,\mkont}
                     \mid \krt{(\mexpr,\menv),\mstore}\\
  \mktab \in \KTab &= (\Expr \times \Env)\times \Store \to \wp(\Kont) \\
  \mmemo \in \Memo &= (\Expr \times \Env)\times \Store \to \wp(\Value)
\end{align*}

\paragraph{The role of summaries:} notice the additional $\Memo$
component. A ``summary edge'' in CFA2 or equivalently an
$\epsilon$-edge in PDCFA is an edge from the source of a push edge to
the target of the matching pop edge. ``Matching'' here means there is
a path through machine reductions that don't change the stack, or
through summary edges. In our model, we only ``push'' when we call a
function, and we only ``pop'' when we return with a value. There is an
analogy to something more operational: summary edges embody
memoization (see the last rule in figure
\ref{fig:summary-semantics}). Instead of following an entire path
through a call to return a value, we simply jump from the call to the
return with the result of the call (the second case of rule 4).

\begin{figure}
  \centering
  $\mstate \stepto \mstate' \text{ where } \maddr = \alloc(\mstate)$ \\
  \begin{tabular}{r|l}
    \hline
% variable lookup
    $\tpl{(\svar\mvar, \menv), \mstore, \mkont, \mktab, \mmemo}$
    &
    $\tpl{\mval,\mstore,\mkont, \mktab, \mmemo}$ if $\mval \in \mstore(\menv(\mvar))$
    \\
% application
    $\tpl{(\sapp{\mexpri0}{\mexpri1}, \menv), \mstore, \mkont, \mktab, \mmemo}$
    &
    $\tpl{(\mexpri0, \menv), \mstore, \kar{\mexpri1,\menv,\mkont}, \mktab, \mmemo}$
    \\
% argument evaluation
    $\tpl{\mval, \mstore, \kar{\mexpr,\menv,\mkont}, \mktab, \mmemo}$
    &
    $\tpl{(\mexpr, \menv), \mstore, \kfn{\mval, \mkont}, \mktab, \mmemo}$
    \\
% function call
    $\tpl{\mval,\mstore,\kfn{\vclo{\slam{\mvar}{\mexpr}}{\menv}, \mkont},\mktab,\mmemo}$
    & % actually do call,
    $\tpl{\mpoint,
          \mstore',
          \krt{\mpoint, \mstore'},
          \mktab',
          \mmemo}$
\\
    & or \\
    & $\tpl{\mval_\mathit{result},
            \mstore',
            \mkont,
            \mktab',
            \mmemo}$
    \\ & \quad if $\mval_\mathit{result} \in \mmemo(\mpoint,\mstore')$
    \\ % or, lookup in table
%    & $\tpl{\mval, \mstore', \mkont,\mktab,\mmemo}$ if $\mval \in \mmemo(\mpoint, \mstore')$ \\    
    where & $\mpoint = (\mexpr, \extm{\menv}{\mvar}{\maddr})$ \\
          & $\mstore' = \joinone{\mstore}{\maddr}{\mval}$ \\
          & $\mktab' = \joinone{\mktab}{(\mpoint, \mstore')}{\mkont}$
    \\
% return
    $\tpl{\mval, \mstore, \krt{\mpoint,\mstore'}, \mktab, \mmemo}$
    &
    $\tpl{\mval, \mstore, \mkont, \mktab, \joinone{\mmemo}{(\mpoint, \mstore')}{\mval}}$
    \\ & \quad if $\mkont \in \mktab(\mpoint, \mstore')$
  \end{tabular}
  \caption{The summarizing tabular stack machine}
  \label{fig:summary-semantics}
\end{figure}

To turn this semantics into PDCFA's algorithm for constructing a Dyck
state graph, given the appropriate $\alloc$, we apply a widening
operator to make $\mstore$, $\mkont$, and $\mmemo$ shared amongst all
states (meaning states then become represented without these
compontents), in what is then called a $\System$.  Since these
components are shared, they are always the least upper bound of any
respective component that the semantics produces (the intermediate set
$I$) in order to stay sound. We use a meta-function $\wn$, ``wide to
narrow'', to shift between the two representations of states.

\begin{align*}
  \mastate \in \sa{State} &= (\Expr \times \Env) \times \Kont \\
  \System &= \wp(\sa{State} \times \Store) \times \wp(\sa{State}) \\
          &\phantom{=} \times \Store \times \KTab \times \Memo \\
  {\mathcal F} &: \System \to \System \\
  {\mathcal F}(S, F, \mstore, \mktab, \mmemo) &= (S \cup S', F', \mstore', \mktab', \mmemo') \\
  \text{where } I &= \setbuild{\mstate'}{\mastate \in F, \wn(\mastate,\mstore,\mktab,\mmemo) \stepto \mstate'} \\
                \mstore' &= \bigsqcup\setbuild{\mstore'}{\wn(\_, \mstore',\_,\_) \in I} \\
                \mktab' &=  \bigsqcup\setbuild{\mktab'}{\wn(\_, \_, \mktab', \_) \in I} \\
                \mmemo' &=  \bigsqcup\setbuild{\mmemo'}{\wn(\_, \_, \_, \mmemo') \in I} \\
                S' &= \setbuild{(\mastate',\mstore')}{\wn(\mastate', \_, \_, \_) \in I} \\
                F' &= \setbuild{\mastate'}{(\mastate',\_) \in S' \setminus S} \\
                \wn(\tpl{(\mexpr,\menv),\mkont},\mstore,\mktab,\mmemo)
                   &= \tpl{(\mexpr,\menv),\mstore,\mkont,\mktab,\mmemo}
\end{align*}

A $\System$ embodies the states seen and at which store, $S$, the
frontier set of states yet to analyze, $F$, the shared store for the
frontier, $\mstore$, the continuation table $\mktab$ and the memo
table $\mmemo$. All the states in the frontier are stepped with the
current store, after which the next store is the least upper bound of
all the resulting stores. The next frontier contains only states
resulting from stepping the previous frontier that we haven't seen at
this next store. Systematic techniques for a performant implementation
can be found in \citet{DBLP:journals/corr/abs-1211-3722}.

\paragraph{Example:} Let's consider a monovariant allocation strategy
(the address for each binding is the variable name itself) to run the
following example:
\begin{lstlisting}[mathescape]
  (let* ([id ($\lambda$ (x) x)]
         [y (id 0)]
         [z (id 1)])
    ($\le$ y z))
\end{lstlisting}
Suppose we extend our semantics to allow numbers, numeric primitives
and \texttt{let}. The continuation frame for a \texttt{let} contains
the identifier to bind to the resulting value, along with the body of
the \texttt{let} with its environment. Call the constructor of this
frame $\mathbf{lt}$.

We should expect that a pushdown analysis would predict this evaluates
to true, and there are no loops in the program. 0CFA ~\citep{ianjohnson:Shivers:1991:CFA} claims there is a
loop from the second call of \texttt{id} to the first, and thusly
predicts this program evaluates to true or false. PDCFA claims there
is no loop, and depending on the implementation, that the result is
true or false (paper), or just true (implemented)\footnote{This is
  because the paper steps every seen state with the current store
  every iteration, but the implementation only steps states that need stepping.}. Let's take a look at the evaluation after binding
\texttt{id} ($\mstore_0 = [\texttt{id} \mapsto
\set{\vclo{\slam{\texttt{x}}{\texttt{x}}}{\bot}}]$):

\begin{enumerate}
\item{\texttt{(id 0)} steps to \texttt{x} at $\krt{((\texttt{x}, [\texttt{x}\mapsto\texttt{x}]), \mstore_0)}$ with $\mstore_1 =
    \mstore_0[\texttt{x} \mapsto \set{0}]$ and
    (let $\mctx_1 = ((\texttt{x},[\texttt{x}\mapsto\texttt{x}]),\mstore_1)$,
         $\mkont_1 = \klet{\texttt{y}, \texttt{(let ([z (id 1)]) ($\le$ y z))}, [\texttt{id}\mapsto{\texttt{id}}]}$)
    $\mktab_1 = [\mctx_0 \mapsto \set{\mkont_0}]$}
\item{\texttt{0} at $\krt{\mctx_1}$ steps to $0$ at $\mkont_1$ and $\mmemo_1 = [\mctx_1 \mapsto \set{0}]$}
\item{\texttt{0} at $\mkont_1$ steps to \texttt{(let ([z (id 1)]) ($\le$ y z))} and $\mstore_2 = \mstore_1[\texttt{y} \mapsto \set{0}]$}
\item{\texttt{(let ([z (id 1)]) ($\le$ y z))} steps to \texttt{(id 1)} at \\
      $\klet{\texttt{z}, \texttt{($\le$ y z)}, [\texttt{id} \mapsto \texttt{id}, \texttt{y} \mapsto \texttt{y}]}$
      (call this $\mkont_2$).}
\item{\texttt{(id 1)} steps to $x$ with $\mstore_3 = \mstore_2[\texttt{x} \mapsto \set{0,1}]$ and
      (let $\mctx_3 = ((\texttt{x},[\texttt{x}\mapsto\texttt{x}]),\mstore_3)$) $\mktab_3 =\mktab_1[\mctx_3 \mapsto \set{\mkont_2}]$.}
\item{\texttt{0} or \texttt{1} at $\krt{\mctx_3}$ steps to \texttt{0} or \texttt{1} at $\mkont_2$ and $\mmemo_3 = \mmemo_1[\mctx_3 \mapsto \set{0,1}]$.}
\item{\texttt{z} gets bound to $\set{0,1}$, and \texttt{($\le$ y z)} evaluates to true.}
\end{enumerate}

We maintained enough context to distinguish the return points of
\texttt{id} to not rebind \texttt{y} to 1. When determining control
flow through the expression, we consult $\mmemo$ to continue past
function calls, so there is no confusion about a back edge from the
second call to \texttt{id}.

\section{Deriving CFA2}
\label{sec:cfa2}

CFA2 is the first published analysis of a higher-order programming
language that could properly match calls and returns. We will show
that it fits well into the same presentation we gave for
PDCFA. Vardoulakis and Shivers had a clear goal of harnessing the
extra information a pushdown model provides to produce a
high-precision analysis that works well in practice. This resulted in
more than just the call/return matching of the previous section, which
is why we are showing the two separately. There are two orthogonal
features of the semantics:
\begin{enumerate}
\item{stack allocation for some bindings in an additional $\mframe \in \Store$}
\item{strong updates on stack frames for resolved non-determinism}
\end{enumerate}
The first of these is an addition to the stack-less context. There is
a conservative pre-analysis that checks locally whether a binding will
never escape, and classifies references (labeled with a distinguishing
$\mlab$ from an arbitrary space of labels) as able to use the stack frame or not. Their criteria for a
binding never escaping is that it is never referenced in a function
that is not its binder. This can be extended in a language with more
linguistic features; see Kranz's thesis about register-allocatable
bindings in the Orbit Scheme compiler
\citep{ianjohnson:kranz:thesis:1988}. A stackable reference is one
that appears in the body of the binding function, by which we mean not
within the body of a nested function. They use the information that a
binding never escapes to not bother allocating it in the heap. This
has the advantage of not changing the heap, and thus leads to less
propagation. The addition of these stack frames makes the analysis
exponential in theory, though in practice they have been observed to
decrease running time in most cases.

The second of these is to ameliorate a problem they call ``fake
rebinding.'' That is, since bindings in the abstract represent several
values, we don't want to reference a variable \texttt{x} in two
different places and have it resolve to two different values. In AAM,
a variable reference non-deterministically steps to all possible
values associated with that variable. Here we want to say that once
\texttt{x} is considered to stand for value $v$, then all subsequent
references of \texttt{x} should be $v$. If they aren't, it looks as if
\texttt{x} was rebound; it hasn't, and thus it is a ``fake
rebinding.'' CFA2 does not step to all values on variable reference,
but instead carries all its values around in superposition until they
need to be observed at, say, a function call. We give a simplified
semantics that is more along the AAM style, but CFA2's approach can
easily be recovered from it\footnote{Our Redex model implements the
  fake rebinding strategy CFA2 itself employs}.

CFA2 uses a ``local semantics'' that is similar to our segmenting
continuations at function boundaries, but gets stuck when it gets to a
point where it would need to ``return.'' It instead appeals to an
external, imperative algorithm for summarization to sew the function
calls and returns together by pattern matching on the states that the
local semantics produces. What we show here is almost wholly the same
in character, only embodied still in terms of a machine semantics and
thus more easily reasoned about, and can be run in the concrete.

We show only the significantly modified rules of the semantics in
figure \ref{fig:frame-semantics}. The other rules simply carry along
the extra $\mframe$ component untouched.
\begin{figure}
  \centering
  $\mstate \stepto \mstate' \text{ where } \maddr = \alloc(\mstate)$ \\
  \begin{tabular}{r|l}
    \hline
% variable lookup
    $\tpl{(\svar[\mlab]{\mvar}, \menv), \mstore, \mframe, \mkont}$
    &
    $\tpl{\mval,\mstore,\mframe',\mkont}$ if $(\mframe', \mval) \in \lookup(\mstore,\mframe,\menv,\mvar,\mlab)$
    \\
% % application
%     $\tpl{(\sapp{\mexpri0}{\mexpri1}, \menv), \mstore, \mframe, \mkont}$
%     &
%     $\tpl{(\mexpri0, \menv), \mstore, \mframe, \kar{\mexpri1,\menv,\mkont}}$
%     \\
% % argument evaluation
%     $\tpl{\mval, \mstore, \mframe, \kar{\mexpr,\menv,\mkont}}$
%     &
%     $\tpl{(\mexpr, \menv), \mstore, \mframe, \kfn{\mval, \mkont}}$
%     \\
% function call
    $\tpl{\mval,\mstore,\mframe,\kfn{\vclo{\slam{\mvar}{\mexpr}}{\menv}, \mkont}}$
    &
    $\tpl{(\mexpr, \extm{\menv}{\mvar}{\maddr}), \mstore', \mframe', \mkont}$
    \\ where & $(\mstore',\mframe') = \bind(\mstore,\mframe,\maddr,\mvar,\mval)$
  \end{tabular}
  \caption{The CES$\xi$K machine}
  \label{fig:frame-semantics}
\end{figure}

The meta-functions the semantics uses to extend and consult the heap
and stack use implicit information from the pre-analysis we described
above:

\begin{align*}
  \bind(\mstore,\mframe,\maddr,\mvar,\mval) &=
   \left\{\begin{array}{l}
            (\mstore, \snglm{\maddr}{\mval}) \text{ if } \mvar \text{ never escapes} \\
            (\joinone{\mstore}{\maddr}{\mval}, \snglm{\maddr}{\mval}) \text{ otherwise} \\
          \end{array}\right. \\
  \lookup(\mstore,\mframe,\menv,\mvar,\mlab) &=
    \left\{\begin{array}{l}
          \setbuild{(\extm{\mframe}{\menv(\mvar)}{\set{\mval}}, \mval)}
                   {\mval \in \mframe(\menv(\mvar))}
                   \\ \qquad \text{if } \mlab \text{ non-escaping} \\
          \setbuild{(\mframe,\mval)}{\mval \in \mstore(\menv(\mvar))} \text{ otherwise}
           \end{array}\right.
\end{align*}

Sidestepping fake rebinding does not need to be restricted to
stackable references, but that is what CFA2 does. Indeed, as soon as
the non-determinism has been determined, we could extend the stack
frame so any subsequent reference means what it meant previously in
the function. Look-up would then always try the stack frame before
falling back on the heap.

CFA2 also includes what they called ``transitive summaries'' to deal
with tail calls. We insert an $\rt$ frame at every function call.
This view contends with tail calls, but we can identify tail calls
easily --- any call with an $\rt$ as its continuation is a tail
call. Tail calls are important in language implementations for space
complexity reasons \citep{ianjohnson:clinger:tail-calls:1998}, but in
an analysis, these concerns are less important. The repeated popping
of $\rt$ inserted by what otherwise were tail-calls is synonymous with
CFA2's transitive summaries.

\paragraph{Example:} if we use this semantics to run through the
example of the previous section, we find that with the addition of
stack frames, there is no confusion about \texttt{z}'s value either,
so if the operator weren't $\le$ but instead $<$ or $+$, we could
still constant fold that away.

\section{Analysis of delimited, composable control}
\label{sec:sr}

There is contention among programming language researchers whether
\rackett{call/cc} should be a language primitive, since it captures
the entire stack, leading to space leaks
\citep{ianjohnson:kiselyov:against-callcc}. Alternative control
operators have been proposed that delimit how much of the stack to
capture, such as $\%$ (read ``prompt'') and capture operator
${\mathcal F}$ (read ``control'')
\citep{ianjohnson:felleisen:control:1988}, or \texttt{reset}
(equivalent to $\%$) and \texttt{shift}
\citep{ianjohnson:danvy:filinski:delim:1990}. However, the stacks
captured by these operators always extend the stack when invoked,
rather than replace it like those captured with
\rackett{call/cc}. Continuations that have this extension behavior are
called ``composable continuations.'' Stack replacement is easily
modeled in a regular analysis using the AAM approach, and Vardoulakis
and Shivers showed it can be done in a pushdown approach (although it
breaks the pushdown model). Stack extension, however, poses a new
challenge for pushdown analysis, since one application of a composable
continuation means pushing an unbounded amount of frames onto the
stack. Vardoulakis' and Shivers' approach does not immediately apply
in this situation, since their technique drops all knowledge of the
stack at a continuation's invocation site; extension, however, must
preserve it.

The way we have been splitting continuations at function calls has
similarities to the meta-continuation approach to modeling delimited
control, given in figure \ref{fig:shift-reset} (adapted from
~\citep{ianjohnson:Biernacki2006274}). We could view each function
call as inserting a prompt, and returns as aborting to the nearest
prompt. Resets then insert a second-tier prompt. The changed semantic
spaces for the shift/reset semantics are as follows:

\begin{align*}
  \mstate \in \State &::= \tpl{\mpoint, \mstore, \mkont, \mmkont} \\
  \mpoint \in \Point &::= (\mexpr, \menv) \mid \mval \\
  \mval \in \Value &::= \vclo{\slam{\mvar}{\mexpr}}{\menv} \mid \vcomp{\mkont} \\
  \mmkont \in \MKont &::= \kmt \mid \mkapp{\mkont}{\mmkont}
\end{align*}

\begin{figure}
  \centering
  $\mstate \stepto \mstate' \text{ where } \maddr = \alloc(\mstate)$ \\
  \begin{tabular}{r|l}
    \hline
% Reset
    $\tpl{(\sreset{\mexpr}, \menv), \mstore, \mkont, \mmkont}$
    &
    $\tpl{(\mexpr, \menv), \mstore, \kmt, \mkapp{\mkont}{\mmkont}}$
    \\
% Pop prompt
    $\tpl{\mval, \mstore, \kmt, \mkapp{\mkont}{\mmkont}}$
    &
    $\tpl{\mval, \mstore, {\mkont}, {\mmkont}}$
    \\
% Shift
    $\tpl{(\sshift{\mvar}{\mexpr}, \menv), \mstore, \mkont, \mmkont}$
    &
    $\tpl{(\mexpr, \extm{\menv}{\mvar}{\maddr}), \mstore',\kmt,\mmkont}$
    \\ & where $\mstore' = \joinone{\mstore}{\maddr}{\mkont}$
    \\
% continuation call
    $\tpl{\mval,\mstore,\kfn{\mkont', \mkont}, \mmkont}$
    &
    $\tpl{\mval, \mstore, \mkont', \mkapp{\mkont}{\mmkont}}$
  \end{tabular}  
  \caption{Machine semantics for shift/reset}
  \label{fig:shift-reset}
\end{figure}

Turning this into a table-based semantics involves making prompts a
point of indirection, just like function calls. Memoization also gets
a new context to consider, calling a continuation, because composable
continuations act like functions. The new semantic spaces are then

\begin{align*}
  \mctx \in \Context &::= ((\mexpr,\menv),\mstore) \mid (\vcomp{\mkont},\mval,\mstore) \\
  \mkont \in \Kont &= \kmt \mid \krt{\mctx} \mid \kar{\mexpr,\menv,\mkont} \mid \kfn{\mval,\mkont} \\
  \mmkont \in \MKont &::= \kmt \mid \kprompt{\mctx} \\
  \mathit{KTabRange} &= \wp(\Kont \times \MKont) \cup \wp(\Kont) \\
  \mktab \in \KTab &= \Context \to \mathit{KTabRange} \\
  \mmemo \in \Memo &= \Context \to \wp(\Value)
\end{align*}

Figure \ref{fig:shift-reset-table0} has what one might naturally write
using just this information. Unfortunately, since $\rt$ frames contain
a store, and continuations can now appear in the store, this
introduces a circularity that could cause the analysis to never
terminate. For example, a loop that continually captures continuations
would introduce unboundedly many continuation values due to the
ever-growing store. This means the store is not finite height, and
there may be no fixed point.

\paragraph{Possible fixes to non-termination:} The simplest fix to the
circularity would be to strip stores out of the context of the $\rt$
frame in a captured continuation, and additionally consider contexts
with a stripped store an abstraction of the same context with any
store. This is a fairly brutal approximation of a captured
continuation, so an alternative is to make this tunable with our
precision-tuning friend, $\alloc$. Then, instead of entirely removing
the store component of the $\rt$ context, we can replace it with an
address of possible stores that it would approximate.

\begin{figure*}
  \centering
  $\mstate \stepto \mstate' \text{ where } \maddr = \alloc(\mstate)$ \\
  \begin{tabular}{r|l}
    \hline
% Reset
    $\tpl{(\sreset{\mexpr}, \menv), \mstore, \mkont, \mmkont, \mktab, \mmemo}$
    &
    $\tpl{\mpoint, \mstore, \kmt, \kprompt{\mctx}, \joinone{\mktab}{\mctx}{(\mkont,\mmkont)}, \mmemo}$
    \\
    where & $\mpoint = (\mexpr, \menv)$, $\mctx = (\mpoint, \mstore)$.
    \\
% Pop prompt
    $\tpl{\mval, \mstore, \kmt, \kprompt{\mctx}, \mktab, \mmemo}$
    &
    $\tpl{\mval, \mstore, {\mkont}, {\mmkont}, \mktab, \joinone{\mmemo}{\mctx}{\mval}}$
    if $(\mkont,\mmkont) \in \mktab(\mctx)$
    \\
% Shift
    $\tpl{(\sshift{\mvar}{\mexpr}, \menv), \mstore, \mkont, \mmkont, \mktab, \mmemo}$
    &
    $\tpl{(\mexpr, \extm{\menv}{\mvar}{\maddr}), \joinone{\mstore}{\maddr}{\vcomp{\mkont}},\kmt,\mmkont,\mktab,\mmemo}$
    \\
% continuation call
    $\tpl{\mval,\mstore,\kfn{\vcomp{\mkont'}, \mkont}, \mmkont, \mktab, \mmemo}$
    &
    $\tpl{\mval, \mstore, \mkont', \kprompt{\mctx}, \mktab',\mmemo}$
    \\
    where & $\mctx = (\mkont', \mval, \mstore)$ \\
          & $\mktab' = \joinone{\mktab}{\mctx}{(\mkont,\mmkont)}$
    \\
% Memoized continuation call
% continuation call
    $\tpl{\mval,\mstore,\kfn{\vcomp{\mkont'}, \mkont}, \mmkont, \mktab, \mmemo}$
    &
    $\tpl{\mval', \mstore, \mkont, \mmkont, \mktab,\mmemo}$ if $\mval' \in \mmemo(\mkont',\mval,\mstore)$
  \end{tabular}  
  \caption{Faulty table-based semantics for shift/reset}
  \label{fig:shift-reset-table0}
\end{figure*}

The new indirection possibility changes $\Context$ to also include
$\maddr$ where there was previously a $\mstore$ (though contexts for
continuation calls remain unchanged), and $\KTab$ now additionally
maps $\Addr$ to a set of stores. The rules that change are presented in figure \ref{fig:shift-reset-table1}.

\begin{align*}
  \mctx \in \Context &::= ((\mexpr,\menv),\mstore) \mid ((\mexpr,\menv),\maddr)
                     \\ & \mid (\vcomp{\mkont},\mval,\mstore) \\
  \msctx \in \SContext &::= ((\mexpr,\menv),\maddr) \\
  \mskont \in \SKont &= \krt{\msctx} \mid \ldots \\
  \mval \in \Value &= \vclo{\slam{\mvar}{\mexpr}}{\menv} \mid \vcomp{\mskont} \\
  \mmkont \in \MKont &::= \kmt \mid \kprompt{\mctx} \\
  \mktab \in \KTab &= (\Context \to \mathit{KTabRange}) \\
                   &\cup (\Addr \to \wp(\Store))
\end{align*}

\begin{figure*}
  \centering
% Shift
  $\mstate \stepto \mstate' \text{ where } \maddr = \alloc(\mstate)$ \\
  \begin{tabular}{r|l}
    \hline
    $\tpl{(\sshift{\mvar}{\mexpr}, \menv), \mstore, \mkont, \mmkont, \mktab, \mmemo}$
    &
    $\tpl{(\mexpr, \extm{\menv}{\mvar}{\maddr}), \joinone{\mstore}{\maddr}{\vcomp{\mkont'}},\kmt,\mmkont,\mktab',\mmemo}$
    \\
    where & $(\mkont',\mktab') = \approximate(\mkont, \mktab, \maddr)$
\\
% Return
   $\tpl{\mval, \mstore, \krt{\mctx}, \mmkont, \mktab, \mmemo}$
   &
   $\tpl{\mval, \mstore, \mkont, \mmkont, \mktab, \mmemo'}$
   if $\mkont \in \returns(\mktab, \mctx)$
   \\
   where & $\mmemo' = \memoize(\mmemo, \mktab, \mctx, \mval)$
  \end{tabular}
  \caption{Fixed table-based semantics for shift/reset}
  \label{fig:shift-reset-table1}
\end{figure*}

The meta-functions mentioned in the fixed semantics all deal with the
addition of $\maddr$ to contexts. If the context in an $\rt$ frame is
approximate, we must return to all the continuations known for all the
contexts it approximates:
\begin{align*}
  \returns(\mktab, ((\mexpr,\menv), \mstore)) &= \mktab((\mexpr,\menv), \mstore) \\
  \returns(\mktab, ((\mexpr,\menv), \maddr)) &=
    \bigcup\setbuild{\mktab((\mexpr,\menv),\mstore)}{\mstore \in \mktab(\maddr)}
\end{align*}

At return boundaries, the memo
table must add the result to all the represented contexts:=
\begin{align*}
  \memoize(\mmemo, \mktab, ((\mexpr,\menv), \mstore), \mval) &=
  \joinone{\mmemo}{((\mexpr,\menv), \mstore)}{\mval} \\
  \memoize(\mmemo, \mktab, ((\mexpr,\menv), \maddr), \mval) &=
  \mmemo \sqcup \bigsqcup\limits_{\mstore \in \mktab(\maddr)}\snglm{((\mexpr,\menv), \mstore)}{\mval}
\end{align*}
At capture time, we strip the store in the $\rt$ frame
if there is one, and replace it with an address:

\newcommand{\replacectx}{\mathit{replace}\mstore}
\newcommand{\addstore}{\mathit{add}\mstore}
\begin{align*}
  \approximate(\mkont, \mktab, \maddr) &= (\replacectx(\mkont), \addstore(\mkont)) \\
  \text{where }
   \replacectx(\kmt) &= \kmt \\
   \replacectx(\krt{((\mexpr,\menv),\_)}) &= \krt{((\mexpr,\menv),\maddr)} \\
   \replacectx(\kar{\mexpr,\menv,\mkont}) &= \kar{\mexpr,\menv,\replacectx(\mkont)} \\
   \replacectx(\kfn{\mval, \mkont}) &= \kfn{\mval,\replacectx(\mkont)}
  \\[2pt]
   \addstore(\kmt) &= \mktab \\
   \addstore(\krt{((\mexpr,\menv),\mstore)}) &= \joinone{\mktab}{\maddr}{\mstore} \\
   \addstore(\krt{((\mexpr,\menv),\maddr')}) &= \joinm{\mktab}{\maddr}{\mktab(\maddr')} \\
   \addstore(\kar{\mexpr,\menv,\mkont}) &= \addstore(\kfn{\mval, \mkont}) = \addstore(\mkont)
\end{align*}

The result of this viewpoint is a sound, precise, and computable
semantics for a ``pushdown approach'' to analyzing delimited,
composable control. To handle non-composable control, we can simply
remove the prompts and keep $\approximate$.

\section{Related Work}

The immediately related work is that of PDCFA
\citep{ianjohnson:earl2010pdcfa},
CFA2~\citep{ianjohnson:vardoulakis-lmcs11,
  ianjohnson:Vardoulakis2011Pushdown}, and AAM
~\citep{dvanhorn:VanHorn2010Abstracting}, the first two of which we
recreated in full detail. The version of CFA2 that handles
\rackett{call/cc} does not handle composable control, and is dependent
on a restricted CPS representation. They also had no way to tune the
precision of their first class continuation approximation. Since
\rackett{call/cc} can be simulated with shift/reset, this work
supercedes theirs. The extended version of PDCFA that does garbage
collection \citep{dvanhorn:Earl2012Introspective} also fits into our
model, although we did not explicitly show it. The addresses that the
stack keeps alive can be accumulated by ``reading through'' the
continuation table, building up the set of addresses in each portion
of the stack that we come across.

Pushdown models have existed in the first-order static analysis
literature ~\citep[Chapter
7]{local:muchnick:jones:flow-analysis:1981}\citep{ianjohnson:reps:pushdown:1995},
and the first-order model checking literature
\citep{ianjohnson:bouajiani:esparza:pushdown:1997}, for some time. The
higher-order setting imposes additional challenges that make their
methods difficult to adapt. The most important constraint is that we
can't know all call-sites of a function/method before the analysis
begins, which their methods heavily rely on.

A new temporal logic that itself understands well-bracketing of call
and returns for stating and validating propositions, NWTL
~\citep{ianjohnson:alur:nwtl:2007}, applies to visibly pushdown
systems, which can precisely model programming languages that only have
well-bracketed call and returns. The propositional form of the logic
has decidable satisfiability checking, and control-flow queries are
shallow propositions to pose in the logic. However, the satisfiability
problem is \textsc{2Exptime}-hard and the algorithm is not given in
the on-the-fly form that a higher-order language would need to
effectively answer control-flow queries without already knowing the
answers. The logic itself is an exciting new frontier for expressing
program correctness properties, and a higher-order version would be a
welcome addition to the functional programmer's tool belt.

The trend of deriving static analyses from abstract machines does not
stop at flow analyses. The model-checking community showed how to
check temporal logic queries for collapsible pushdown automata (CPDA),
or equivalently, higher-order recursion schemes, by deriving the
checking algorithm from the Krivine
machine~\citep{ianjohnson:Salvati:2011:KMH:2027223.2027239}. The
expressiveness of CPDAs outweighs that of PDAs, but it is unclear how
to adapt higher-order recursion schemes to model arbitrary programming
language features. The method is strongly tied to the simply-typed
call-by-name lambda calculus and depends on finite sized base-types.

\section{Conclusion and future work}

As the programming world continues to embrace behavioral values, it
becomes more important to import the powerful techniques pioneered by
the first-order analysis and model checking communities. CFA2 and
PDCFA paved the way, and in large part inspired this work. It is our
view that systematic approaches to applying the techniques are pivotal
to scaling them to ``real languages.'' We believe that the recipe that
this paper set forth is a step in that direction. That is, make
continuation tables keyed with enough context, and memoize at the
introduced indirection points. The result in a language with
well-bracketed control is a ``pushdown analysis'' using
summarization. In a language without well-bracketed control, we are
not chained to a pushdown automaton as the target of the
approximation, so the techniques still apply and give better precision
than regular methods.

Our goal in the future is to show that this technique is even more
widely applicable than shift and reset. We conjecture that the same
recipe will apply to the most intricate control operators in
production languages such as in
\citet{ianjohnson:Flatt:2007:ADC:1291151.1291178}. The control
structures there are difficult to model even with AAM's techniques due
to the ability to capture and compose continuations with arbitrarily many prompts,
but once we can tackle AAM, there should be a straightforward route to
extending it with the pushdown techniques of this paper.

There is also the question of clients of these analyses. Most
obviously we would want to know where we can implement first-class
control more efficiently. In particular, we would want to have an
escape analysis to find first-class continuations that don't need to
be heap-allocated, and single target abort operations that can be
turned into long jumps after a computed stack unraveling. On top of
optimizations, there are security analyses. Greater control of the
stack gives us the ability to drill deep into context-sensitive
security properties and make precise predictions.

\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}